{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2b4e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŒŸ Multi-Class Classification\n",
    "# Multi-Class Classification is a type of Supervised Machine Learning used when the problem requires assigning an input data point to one of three or more discrete categories. It is an extension of Binary Classification, which only handles two classes.\n",
    "\n",
    "# Core Concept :The goal is still to learn decision boundaries in the feature space, but instead of one boundary separating two classes, the model must define multiple boundaries to separate all N classes. When presented with a new input, the model outputs a predicted class label, which is one of the N possible categories.\n",
    "# The mathematical approach often used by algorithms like Logistic Regression and Support Vector Machines (SVMs) to handle multi-class problems is:\n",
    "# One-vs-Rest (OvR) or One-vs-All (OvA): This strategy trains N separate binary classifiers, where N is the number of classes. Each classifier is trained to distinguish one class from all the other classes combined. For a new input, all N classifiers make a prediction, and the one with the highest confidence score wins.\n",
    "# One-vs-One (OvO): This trains {N(N-1)}/2 binary classifiers, one for every unique pair of classes. For a new input, the class that receives the most \"votes\" from the binary classifiers is chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a13e0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example uses the popular Iris dataset, a classic benchmark in machine learning, to classify flower samples into one of three species: Setosa, Versicolor, or Virginica. We will use the K-Nearest Neighbors (KNN) algorithm, which naturally supports multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804430c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f2fa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Load and Prepare Multi-Class Data ---\n",
    "# Load the Iris dataset, which has 3 classes (0, 1, 2)\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)  # Features\n",
    "y = iris.target  # Target classes (0, 1, 2)\n",
    "\n",
    "# Select only two features for easier visualization: Petal Length and Petal Width\n",
    "X_2d = X[[\"petal length (cm)\", \"petal width (cm)\"]]\n",
    "target_names = iris.target_names  # ['setosa', 'versicolor', 'virginica']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_2d, y, test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1792607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Train the Model ---\n",
    "# K-Nearest Neighbors (KNN) is an effective classifier for this task\n",
    "model = KNeighborsClassifier(\n",
    "    n_neighbors=5\n",
    ")  # K=5 is the number of nearest neighbors to check\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cecccd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Make Predictions ---\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8438311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Evaluate the Model ---\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(pd.DataFrame(conf_matrix, index=target_names, columns=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceff0a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Example Prediction ---\n",
    "# Predict the species for a new flower with (Petal Length=4.5cm, Petal Width=1.5cm)\n",
    "# Use a DataFrame with the same column names used during training to avoid sklearn warnings\n",
    "new_flower = pd.DataFrame([[4.5, 1.5]], columns=X_2d.columns)\n",
    "prediction_class = model.predict(new_flower)[0]\n",
    "predicted_species = target_names[prediction_class]\n",
    "\n",
    "print(\n",
    "    f\"New Flower Prediction (4.5cm, 1.5cm): Class {prediction_class} ({predicted_species})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303a20d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code continues from the previous block using the trained 'model', 'X_2d', and 'y'\n",
    "\n",
    "# Define the range for each feature with a small margin\n",
    "pl_min, pl_max = X_2d.iloc[:, 0].min() - 0.5, X_2d.iloc[:, 0].max() + 0.5\n",
    "pw_min, pw_max = X_2d.iloc[:, 1].min() - 0.5, X_2d.iloc[:, 1].max() + 0.5\n",
    "\n",
    "# Create a grid of points (e.g., 200x200)\n",
    "xx, yy = np.meshgrid(np.linspace(pl_min, pl_max, 200), np.linspace(pw_min, pw_max, 200))\n",
    "\n",
    "# Predict classes for the entire grid\n",
    "Z_input = np.c_[xx.ravel(), yy.ravel()]\n",
    "# Convert grid to DataFrame with the same feature names used in training to avoid sklearn warnings\n",
    "Z_input_df = pd.DataFrame(Z_input, columns=X_2d.columns)\n",
    "Z = model.predict(Z_input_df)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Visualize the Multi-Class Boundaries and Data Points\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the colored regions (the three decision regions)\n",
    "plt.contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.viridis)\n",
    "\n",
    "# Plot the actual data points, colored by their true class\n",
    "scatter = plt.scatter(\n",
    "    X_2d.iloc[:, 0], X_2d.iloc[:, 1], c=y, edgecolors=\"k\", cmap=plt.cm.viridis, s=50\n",
    ")\n",
    "\n",
    "# Create a robust legend for the classes using proxy handles\n",
    "\n",
    "n_classes = len(np.unique(y))\n",
    "cmap = plt.cm.viridis\n",
    "colors = cmap(np.linspace(0, 1, n_classes))\n",
    "handles = [\n",
    "    Line2D(\n",
    "        [0],\n",
    "        [0],\n",
    "        marker=\"o\",\n",
    "        color=\"w\",\n",
    "        markerfacecolor=colors[i],\n",
    "        markeredgecolor=\"k\",\n",
    "        markersize=8,\n",
    "    )\n",
    "    for i in range(n_classes)\n",
    "]\n",
    "plt.legend(\n",
    "    handles=handles, labels=list(target_names), title=\"True Class\", loc=\"lower right\"\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Petal Length (cm)\", fontsize=12)\n",
    "plt.ylabel(\"Petal Width (cm)\", fontsize=12)\n",
    "plt.title(\"KNN Multi-Class Classification Decision Boundaries (Iris Dataset)\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
